import time

import keras.models

from FuncionesAux import *
import os

#a = [0.9341790330301579, 0.9431243680485338, 0.9302264349657715, 0.8976063829787234, 0.7612994350282486, 0.8027681660899654, 0.883422778771616, 0.8678996036988111, 0.8940903054448871, 0.8021390374331551, 0.7011577424023154, 0.8203497615262321, 0.6885826771653544, 0.7694805194805194, 0.8699127906976745, 0.9385065885797951, 0.6821460775473399, 0.6757446808510639, 0.625560538116592, 0.6812730627306273, 0.6862292051756007, 0.7629464285714286, 0.6812267657992565, 0.733275412684622, 0.8276353276353277, 0.7145631067961165, 0.7737512242899118, 0.6461100569259962, 0.6484536082474227, 0.7703804347826086, 0.692495126705653, 0.8100158982511924, 0.9517839922854388, 0.6974874371859296, 0.7404737384140062, 0.7182497331910352, 0.6921397379912664, 0.83991683991684, 0.7159090909090909, 0.7304123711340206, 0.819593147751606, 0.7406143344709898, 0.7529923830250272, 0.9030511811023622, 0.794928335170893, 0.8336734693877551, 0.7694444444444445, 0.813169164882227, 0.795766590389016, 0.677639046538025, 0.7776427703523694, 0.6680622009569378, 0.7494331065759637, 0.6597877358490566, 0.6234498308906427, 0.7119883040935673, 0.5963517305893359, 0.6025157232704402, 0.7271604938271605, 0.8811428571428571, 0.8847980997624703, 0.911619283065513, 0.7425925925925926, 0.7534403669724771, 0.889967637540453, 0.8218673218673219, 0.8141809290953546, 0.8023529411764706, 0.7348866498740554, 0.7303149606299213, 0.9146706586826348, 0.6721204188481675, 0.728161668839635, 0.7363861386138614, 0.6814621409921671, 0.6974900924702774, 0.8816091954022989, 0.7414733969986358, 0.6356749311294766, 0.6644113667117727, 0.7728426395939086, 0.6990291262135923, 0.6681286549707602, 0.7363861386138614, 0.8609226594301221, 0.6989869753979739, 0.9851428571428571, 0.8656821378340366, 0.7958152958152959, 0.6799709724238027, 0.8404255319148937, 0.8150584795321637, 0.6676646706586826, 0.8350456621004566, 0.8816234498308907, 0.7659090909090909, 0.698948948948949, 0.7159810126582279, 0.7072649572649573, 0.7840059790732437, 0.7254901960784313, 0.7856115107913669, 0.7263349514563107, 0.7727987421383647, 0.6445161290322581, 0.8933649289099526, 0.8597922848664689, 0.843167701863354, 0.7211394302848576, 0.7975077881619937, 0.8182552504038773, 0.8044776119402985, 0.7742759795570698, 0.7466101694915255, 0.668046357615894, 0.8368678629690048, 0.7643478260869565, 0.8093354430379747, 0.8212765957446808, 0.8462732919254659, 0.9453471196454948, 0.661344537815126, 0.9590268886043534, 0.8333333333333334, 0.7545551982851019, 0.8147826086956522, 0.7825443786982249, 0.9396984924623115, 0.786053882725832, 0.7550675675675675, 0.9046052631578947, 0.9388984509466437, 0.7896081771720613, 0.7797513321492007, 0.8047882136279927, 0.7819253438113949, 0.7811355311355311, 0.7618613138686131, 0.9233511586452763, 0.6892138939670932, 0.9115456238361266, 0.8166058394160584, 0.6924428822495606, 0.7763819095477387, 0.8016917293233082, 0.9301270417422868, 0.7280701754385965, 0.809106830122592, 0.9330645161290323, 0.7843137254901961, 0.7400379506641366, 0.8026819923371648, 0.9351851851851852, 0.874455732946299, 0.9143126177024482, 0.8163841807909604, 0.835216572504708, 0.8690248565965584, 0.69921875, 0.831511839708561, 0.8044806517311609, 0.8757225433526011, 0.7893258426966292, 0.8358490566037736, 0.873083475298126, 0.9253308128544423, 0.8151950718685832, 0.6856287425149701, 0.8663551401869158, 0.8731656184486373, 0.8458188153310104, 0.8014128728414442, 0.7626050420168067, 0.7111486486486487, 0.7060041407867494, 0.8934262948207171, 0.7995867768595041, 0.8224852071005917, 0.8313492063492064, 0.7058823529411765, 0.8774900398406374, 0.8821362799263351, 0.8471400394477318, 0.8303571428571429, 0.9198218262806236, 0.7394366197183099, 0.8354430379746836, 0.9234042553191489, 0.8482824427480916, 0.9267515923566879, 0.929042904290429, 0.9311827956989247, 0.7842465753424658, 0.8561320754716981, 0.7934362934362934, 0.7523629489603024, 0.8767942583732058, 0.9251207729468599, 0.785377358490566, 0.789179104477612, 0.9172043010752688, 0.7956521739130434, 0.7907542579075426, 0.8186157517899761, 0.9486166007905138, 0.8121951219512196, 0.8801369863013698, 0.8669833729216152, 0.7463235294117647, 0.9172494172494172, 0.8929411764705882, 0.9701789264413518, 0.8856132075471698, 0.9055299539170507, 0.8154450261780105, 0.8634020618556701, 0.7897091722595079, 0.8464467005076142, 0.8269720101781171, 0.8148558758314856, 0.8890649762282092, 0.8018617021276596, 0.8258547008547008, 0.8981958762886598, 0.7300613496932515, 0.9110824742268041, 0.7986512524084779, 0.9091903719912473, 0.9017341040462428, 0.7507002801120448, 0.7576177285318559, 0.8737270875763747, 0.8108465608465608, 0.9263565891472868, 0.8870967741935484, 0.8923076923076924, 0.8729281767955801, 0.788135593220339, 0.8577235772357723, 0.8170028818443804, 0.9140401146131805, 0.9263005780346821, 0.7236842105263158, 0.8304347826086956, 0.8352272727272727, 0.9162234042553191, 0.9344660194174758, 0.9052924791086351, 0.9042553191489362, 0.7956204379562044, 0.8946540880503144, 0.7185714285714285, 0.9205202312138728, 0.9287790697674418, 0.8459214501510574, 0.7957957957957958, 0.8668407310704961, 0.7943786982248521, 0.8455414012738853, 0.773371104815864, 0.9088050314465409, 0.8997134670487106, 0.85828025477707, 0.8935185185185185, 0.8964968152866242, 0.9210526315789473, 0.8454258675078864, 0.8862275449101796, 0.8322981366459627, 0.9511400651465798, 0.789198606271777, 0.8971119133574007, 0.9275618374558304, 0.8976510067114094, 0.8949152542372881, 0.8925729442970822, 0.7147766323024055, 0.9354838709677419, 0.822992700729927, 0.9130434782608695, 0.9247648902821317, 0.8863636363636364, 0.8067226890756303, 0.8556910569105691, 0.7941176470588235, 0.805019305019305, 0.874, 0.8617021276595744, 0.9761092150170648, 0.9013157894736842, 0.9681647940074907, 0.8427230046948356, 0.9074733096085409, 0.8671171171171171, 0.8484848484848485, 0.8068783068783069, 0.8825136612021858, 0.9325581395348838, 0.8582089552238806, 0.900523560209424]
#print(sum(a)/len(a))

UM_t = []
accuracy_t = []
k=1
#dataset = leer_cliente_veremi_ex(k)  # veremi
#print(dataset)
#dataset = pd.read_csv(dataset)
#dataset = pd.read_csv("/home/enrique/flower/Autoencoder_limpio/Veremi_extension/data_party1.csv")
dataset = pd.read_csv("/home/enrique/Datasets/Veremi extension/csvs/prueba.csv")
print("DISPOSITIVO " + str(k))
# dataset = pd.read_csv('/home/user/flower/sklearn-logreg-mnist/MUD_mlp/SMOTETomek/dispositivo_'+str(k)+'.csv') #otro

scaler = MinMaxScaler()
features_to_normalize = dataset.columns.difference(['Label'])
dataset[features_to_normalize] = scaler.fit_transform(dataset[features_to_normalize])

pd.set_option('use_inf_as_na', True)
dataset.fillna(dataset.median(), inplace=True)
dataset = dataset.sample(frac=1).reset_index(drop=True)
useless = ['Unnamed: 0','type',				'pos_2'	,		'pos_noise_2'	,		'spd_2'	,		'spd_noise_2'		,	'acl_2'	,		'acl_noise_2'		,	'hed_2'	,
                'hed_noise_2', 'sender',		'senderPseudo',	'messageID',]
dataset.drop(useless, axis=1, inplace=True)

cv_type = "diag"
n_component = 20
Normal = dataset[dataset.Label == 0]
Normal = Normal.sample(n=500)
print(len(Normal))
A = dataset[dataset.Label > 0]
#A = A.sample(n=5000)
print(len(A))

useless = ['Label']


"""
Normal.drop(useless, axis=1, inplace=True)
A.drop(useless, axis=1, inplace=True)
Normal=np.array(Normal)
n = int(0.8+len(Normal))
normal_train = Normal[0:n]
normal_test = Normal[n:]
n_inputs = normal_train.shape[1]
#mitad_layers = [n_inputs, 200, 100]
mitad_layers = [n_inputs, int(n_inputs/2), int(n_inputs/3)]
model = create_autoencoder_rbm(normal_train, mitad_layers)
model.fit(normal_train, normal_train, epochs=30)
"""
  # 244
# n_component = 142

# n_component = componentes[lc_ind]


inicio = time.time()
Normal_t, A_t, hist, desv, centers, gmm = create_histogram_veremi(Normal, A, n_component, cv_type)
n_inputs_mud = Normal.shape[1]


def sample(args):
    z_mean, z_log_var = args
    batch = K.shape(z_mean)[0]
    dim = K.int_shape(z_mean)[1]
    epsilon = K.random_normal(shape=(batch, dim))
    return z_mean + K.exp(0.5 * z_log_var) * epsilon



original_dim = hist.shape[1]
input_shape = (original_dim,)
intermediate_dim = int(original_dim / 2)
latent_dim = int(original_dim / 3)

# encoder model
inputs = Input(shape=input_shape, name='encoder_input')
x = Dense(intermediate_dim, activation='relu')(inputs)
z_mean = Dense(latent_dim, name='z_mean')(x)
z_log_var = Dense(latent_dim, name='z_log_var')(x)
# use the reparameterization trick and get the output from the sample() function
z = Lambda(sample, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])
encoder = Model(inputs, z, name='encoder')
# encoder.summary()

# decoder model
latent_inputs = Input(shape=(latent_dim,), name='z_sampling')
x = Dense(intermediate_dim, activation='relu')(latent_inputs)
outputs = Dense(original_dim, activation='sigmoid')(x)
# Instantiate the decoder model:
decoder = Model(latent_inputs, outputs, name='decoder')
# decoder.summary()

# full VAE model
outputs = decoder(encoder(inputs))
vae_model = Model(inputs, outputs, name='vae_mlp')


# the KL loss function:
def vae_loss(x, x_decoded_mean):
    # compute the average MSE error, then scale it up, ie. simply sum on all axes
    reconstruction_loss = K.sum(K.square(x - x_decoded_mean))
    # compute the KL loss
    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.square(K.exp(z_log_var)), axis=-1)
    # return the average loss over all
    total_loss = K.mean(reconstruction_loss + 0.001 * kl_loss)
    # total_loss = reconstruction_loss + kl_loss
    return total_loss


from tensorflow import optimizers

# opt = optimizers.Adam(learning_rate=0.0001, clipvalue=0.5)
set_lr(0.1)
opt = optimizers.RMSprop(learning_rate=0.005)
vae_model.compile(optimizer=opt, loss=vae_loss)
for i in range(30):
    vae_model.fit(hist, hist, epochs=1, verbose=3)

    Hpred = pd.DataFrame(vae_model.predict(hist))
    REC = np.sqrt(np.sum((hist - Hpred) ** 2, axis=1))
    UM = np.mean(REC) + 0.001 * np.std(REC)
    #print(UM)
    n = int(len(Normal_t))
    # print(n)
    accuracy, recall, precision, f1_sc, mcc, acc_ae, p_ae, acc_gmm, p_gmm = my_metrics(Normal_t, A_t, n_inputs_mud, UM, n, vae_model,
                                                         centers,
                                                         desv, gmm, n_component,1)
    accuracy_t.append(accuracy)
    #model.save('/home/enrique/flower/Autoencoder_Veremi/modelos/modelo_5.h5')
    print("PARTY ACCURACY: " + str(accuracy))
    print("accuracy gmm: ",acc_gmm)
    print("accuracy vae: ", acc_ae)
    print("proporcion: ", p_ae)
print(accuracy_t)
